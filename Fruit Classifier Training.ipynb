{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a30777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "import torchvision.transforms.v2 as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f900368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Class labels\n",
    "DATA_LABELS = [\n",
    "    \"freshapples\", \"freshbanana\", \"freshoranges\",\n",
    "    \"rottenapples\", \"rottenbanana\", \"rottenoranges\"\n",
    "]\n",
    "N_CLASSES = len(DATA_LABELS)\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42752df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms for training and validation\n",
    "train_transforms = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(20),\n",
    "    T.ColorJitter(0.3, 0.3, 0.3),\n",
    "    T.ToImage(),\n",
    "    T.ToDtype(torch.float32, scale=True),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToImage(),\n",
    "    T.ToDtype(torch.float32, scale=True),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "422fdefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for idx, label in enumerate(DATA_LABELS):\n",
    "            img_paths = glob.glob(os.path.join(root_dir, label, \"*.png\"))\n",
    "            for path in img_paths:\n",
    "                self.imgs.append(path)\n",
    "                self.labels.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(self.imgs[idx], ImageReadMode.RGB)\n",
    "        image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6dc4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading paths\n",
    "train_path = \"C:/Users/okeiy/Downloads/Nvdia Learning/dataset/train\"\n",
    "val_path = \"C:/Users/okeiy/Downloads/Nvdia Learning/dataset/test\"\n",
    "\n",
    "train_dataset = FruitDataset(train_path, train_transforms)\n",
    "val_dataset = FruitDataset(val_path, val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bebd6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model using VGG16 as base\n",
    "vgg_model = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "vgg_model.requires_grad_(True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    vgg_model.features,\n",
    "    vgg_model.avgpool,\n",
    "    nn.Flatten(),\n",
    "    vgg_model.classifier[0:3],\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(4096, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(500, N_CLASSES)\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e82ac02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "def train(model, loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Train - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "903fcbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation loop\n",
    "def validate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fc70fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Train - Loss: 0.1778, Accuracy: 94.01%\n",
      "Validation - Loss: 0.0565, Accuracy: 98.41%\n",
      "\n",
      "Epoch 2/10\n",
      "Train - Loss: 0.0645, Accuracy: 97.99%\n",
      "Validation - Loss: 0.0193, Accuracy: 99.48%\n",
      "\n",
      "Epoch 3/10\n",
      "Train - Loss: 0.0310, Accuracy: 99.16%\n",
      "Validation - Loss: 0.0618, Accuracy: 98.41%\n",
      "\n",
      "Epoch 4/10\n",
      "Train - Loss: 0.0495, Accuracy: 98.65%\n",
      "Validation - Loss: 0.1756, Accuracy: 96.26%\n",
      "\n",
      "Epoch 5/10\n",
      "Train - Loss: 0.0330, Accuracy: 99.15%\n",
      "Validation - Loss: 0.0027, Accuracy: 99.93%\n",
      "\n",
      "Epoch 6/10\n",
      "Train - Loss: 0.0353, Accuracy: 99.09%\n",
      "Validation - Loss: 0.0672, Accuracy: 98.37%\n",
      "\n",
      "Epoch 7/10\n",
      "Train - Loss: 0.0237, Accuracy: 99.39%\n",
      "Validation - Loss: 0.0026, Accuracy: 99.96%\n",
      "\n",
      "Epoch 8/10\n",
      "Train - Loss: 0.0462, Accuracy: 98.87%\n",
      "Validation - Loss: 0.0290, Accuracy: 99.33%\n",
      "\n",
      "Epoch 9/10\n",
      "Train - Loss: 0.0330, Accuracy: 99.18%\n",
      "Validation - Loss: 0.0257, Accuracy: 99.11%\n",
      "\n",
      "Epoch 10/10\n",
      "Train - Loss: 0.0147, Accuracy: 99.60%\n",
      "Validation - Loss: 0.0111, Accuracy: 99.67%\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    validate(model, val_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d1b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"fruit_model_1.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla-sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
